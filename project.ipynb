{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMkkesApLfe5wp5SCkUrf0a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Giridhar-github/project/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "j8l25e4VMB6O"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "\n",
        "#LIBROSA IS A PYTHON LIBRARY FOR ANALYZING AUDIO AND MUSIC\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#TO PLAY THE AUDIO FILES\n",
        "\n",
        "from IPython.display import Audio\n",
        "import keras\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import wave\n",
        "import pylab\n",
        "from pathlib import Path\n",
        "from scipy import signal\n",
        "from scipy.io import wavfile\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import pylab\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPool2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "import warnings\n",
        "if not sys.warnoptions:\n",
        "  warnings.simplefilter(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Ravdess = \"https://www.kaggle.com/uwrfkaggler/ravdess-emotional-speech-audio\"\n",
        "CremaD = \"/kaggle/input/cremad/AudioWAV/\"\n",
        "Tess = \"/kaggle/input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/\"\n",
        "Savee = \"/kaggle/input/savee-database/AudioData\""
      ],
      "metadata": {
        "id": "gMdTxAnWQK3L"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ravdess=os.listdir(Ravdess)\n",
        "\n",
        "emotion=[]\n",
        "path=[]\n",
        "gender=[]\n",
        "intensity=[]\n",
        "for i in ravdess:\n",
        "  actor=os.listdir(Ravdess+i)\n",
        "  for f in actor:\n",
        "    fsplit=f.split('.')[0]\n",
        "    fsplit=fsplit.split('-')\n",
        "    emotion.append(int(fsplit[2]))\n",
        "    gend=int(fsplit[6])\n",
        "    if gender%2==0:\n",
        "      gender.append('Female')\n",
        "    else:\n",
        "      gender.append('Male')\n",
        "    intensity.append(int(fsplit[3]))\n",
        "    for sp in emotion:\n",
        "      if sp=='1':\n",
        "        intensity.append('Normal')\n",
        "      path.append(Ravdess + i +'/' + f)\n",
        "edf=pd.DataFrame(emotion, columns=['Emotion'])\n",
        "pdf=pd.DataFrame(path, columns=['Path'])\n",
        "gdf=pd.DataFrame(gender, columns=['Gender'])\n",
        "idf=pf.DataFrame(intensity, columns=['intensity'])\n",
        "\n",
        "ravdess_df=pd.concat([edf, pdf, gdf, idf], axis=1)\n",
        "ravdess_df.Emotions.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\n",
        "ravdess_df.head()\n",
        "ravdess_df.isnull()\n",
        "ravdess_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "RnHk2LR2QU9K",
        "outputId": "e8d69dd1-dae7-4dbf-8959-1140dfefd03e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-2d249cebfe9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mravdess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRavdess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0memotion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'https://www.kaggle.com/uwrfkaggler/ravdess-emotional-speech-audio'"
          ]
        }
      ]
    }
  ]
}